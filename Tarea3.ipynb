{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf04bd40-42df-46b4-8149-2e8ccc515c68",
   "metadata": {},
   "source": [
    "# Tarea 3: Introducción al análisis y procesamiento de audio con python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c208e2e-02f0-4b2c-828f-92f42f6e86fe",
   "metadata": {},
   "source": [
    "## Análisis de audio con Python y Jupyterlab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fc248c-d8d3-4447-95b5-c848e20d6bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos cada una de las librerias que necesitamos para realizar la tarea.\n",
    "from scipy.io import wavfile\n",
    "import IPython\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a5224b-2321-4f8e-902c-c713c03ef6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aquí definimos los directorios donde guardaremos los audios con los que vamos a trabajar.\n",
    "cwd = os.getcwd()\n",
    "audio_input_path = os.path.join(cwd, os.path.join('audio', '_input'))\n",
    "audio_output_path = os.path.join(cwd, os.path.join('audio', '_output'))\n",
    "print(f'Directorio con los audios de entrada: {audio_input_path}')\n",
    "print(f'Directorio donde guardaremos los audios generados: {audio_output_path}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afedc296-f545-4a74-917b-c4ab0bee6cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el archivo de audio .wav en este caso.\n",
    "filename = os.path.join(audio_input_path, 'breaking_bad.wav')\n",
    "\n",
    "sample_rate_1, audio_data = wavfile.read(filename)\n",
    "print(f'Frecuencia de muestreo (sample rate_1): {sample_rate_1/1000} kHz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f3c2e0-51aa-478d-b131-37145b0531f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este caso, mostramos el audio que hemos cargado.\n",
    "IPython.display.Audio(audio_data.T, rate=sample_rate_1) # .T se pasa únicamente si es audio estéreo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6655f4e9-24b8-4ca4-8fe1-5dae56cf6cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este punto se calcula cada una de las características del audio 'estereo'.\n",
    "print('Datos de audio (estereo):')\n",
    "print(f'- Tamaño:     {audio_data.shape}')\n",
    "print(f'- 1º canal:   {audio_data[:5, 0]}...')\n",
    "print(f'- 2º canal:   {audio_data[:5, 1]}...')\n",
    "print(f'- Resolucion: {type(audio_data[0,0])}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161d7406-f3fd-4191-8582-6a4037ba82de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertimos a mono mediante la media por canal (simplificacion) y mostramos las características del audio mono.\n",
    "new_data_mono = audio_data.mean(axis=1)\n",
    "print('Nuevos datos de audio (mono):')\n",
    "print(f'- Nuevo tamaño: {new_data_mono.shape}')\n",
    "print(f'- Canal unico:  {new_data_mono[:5]}...')\n",
    "\n",
    "# Mantenemos la misma resolucion que antes.\n",
    "new_data_mono = new_data_mono.astype(np.int16)\n",
    "print(f'- Resolucion:   {type(new_data_mono[0])}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f698a9-5735-4c87-947d-4cbd8146c65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos el archivo mono a un fichero de tipo wav.\n",
    "wavfile.write(\n",
    "    filename=os.path.join(audio_output_path, 'breaking_bad_mono.wav'),\n",
    "    rate=sample_rate_1,\n",
    "    data=new_data_mono\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e3e4d2-6cdd-4d2b-a247-c6cf2dfffb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este caso, mostramos el audio mono que hemos cargado.\n",
    "IPython.display.Audio(new_data_mono, rate=sample_rate_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ad21c2-de75-4cf3-a623-79ac558cab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este punto se muestran las diferencias de tamaño entre cada punto.\n",
    "!ls -sh audio/_input/breaking_bad.wav\n",
    "!ls -sh audio/_output/breaking_bad_mono.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5753a7-5fe3-4ced-8f54-b2c346c0e44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Además, mostramos la frecuencia de muestreo del archivo de audio mono.\n",
    "print(f'Frecuencia de muestreo (sample rate): {sample_rate_1/1000} kHz\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8b17f1-00ff-42a6-82f4-21d3d0cad3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este punto, mostramos el número de muestras del audio estéreo y mono que se van a coger para realizar la grafica en el dominio del tiempo. \n",
    "ampl_values_1 = len(audio_data)\n",
    "ampl_values_2 = len(new_data_mono)\n",
    "print(f'Número de muestras del audio estereo (valores de amplitud): {ampl_values_1}')\n",
    "print(f'Número de muestras del audio mono (valores de amplitud): {ampl_values_2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aed54c2-5c78-44cc-8482-68e7dd40e10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A continuación, se construye un array que representa el eje 'X' del tiempo en la \n",
    "# grabación de audio, para ello se utiliza la función numpy.arange(). \n",
    "t1 = np.arange(0, ampl_values_1/sample_rate_1, 1/sample_rate_1)\n",
    "t2 = np.arange(0, ampl_values_2/sample_rate_1, 1/sample_rate_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134506da-a42a-4147-88b3-908726cd1193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se imprimen los valores de los arrays t1 y t2, que contienen los instantes de tiempo en los que se tomaron las muestras de la señal de audio.\n",
    "print(t1)\n",
    "print(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fbf7c7-11a0-4a8a-9e25-fc5aef8cc36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por último, se crea una figura con dos gráficos que muestran la señal de audio en el dominio del tiempo para dos tasas de muestreo diferentes.\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 6), sharex=True)\n",
    "\n",
    "# Solo mostramos las primeras 50 muestras de amplitud (por claridad).\n",
    "end = 50\n",
    "\n",
    "# Señal a 48 kHz.\n",
    "ax[0].plot(t1[:end], audio_data[:end], marker='X')\n",
    "ax[0].set_title(f'Audio en el dominio del tiempo muestreado a {sample_rate_1} Hz')\n",
    "ax[0].set_ylabel('Amplitud')\n",
    "ax[0].grid(True)\n",
    "\n",
    "# Señal a 24 kHz.\n",
    "# Utilizamos ratio para ajustar el eje x de ambas gráficas\n",
    "# ya que la fs es menor en esta señal.\n",
    "ratio = sample_rate_1 / sample_rate_1 \n",
    "ax[1].plot(t2[:int(end/ratio)], new_data_mono[:int(end/ratio)], c='tab:red', marker='X')\n",
    "ax[1].set_title(f'Audio en el dominio del tiempo muestreado a {sample_rate_1} Hz')\n",
    "ax[1].set_xlabel('Tiempo (s)')\n",
    "ax[1].set_ylabel('Amplitud')\n",
    "ax[1].grid(True)\n",
    "\n",
    "# Mostramos la figura.\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9face91c-8ad9-40a0-a619-7e9bac1050e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prueba",
   "language": "python",
   "name": "prueba"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
